---
title: "Practical Machine Learning - Project"
author: "Suresh"
date: "November 27, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Background

The data that is provided for the assignment is the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).  The training data has the the variable 'classe' which defines how well the exercise was performed.  

Our outcome variable is classe, a factor variable with 5 levels. For this data set, "participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

* exactly according to the specification (Class A)
* throwing the elbows to the front (Class B)
* lifting the dumbbell only halfway (Class C)
* lowering the dumbbell only halfway (Class D)
* throwing the hips to the front (Class E)?

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

The goal of your project is to predict the manner in which they did the exercise.  The prediction needs to be done on the 20 cases in the testing data set.
The training data for this project are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har]. 

## Summary of analysis

The steps to analyse the data and do the prediction are as follows

* Download the provided datasets and load into R
* Do some exploratory data analysis on the data to under the structure and issues
* Clean up the data to remove columns having NAs, variables which add no value to the analysis and variables which are little or no variance.
* Split the training dataset into training and testing datasets to do cross validation.
* Rin the Classification Tree and Random Forest methods on the training dataset and create the models.
* Do predictions for each model and geneate the accuracy metrics.
* Using the model with the better accuracy, run prediction on the testing dataset provided and do the prediction for the test cases.

## Download the data and import necessary libraries
```{r}
#Get libraries needed
library(caret)

#Download the data and read into R
modelDf <- read.csv("pml-training.csv")
predictDf <- read.csv("pml-testing.csv")

```

##Exploratory Data Analysis

```{r}
summary(modelDf$classe)

#Check how many columns have all NA values
sum(colSums(is.na(modelDf)) == 0)

#Check how many columns have no variance or near zero variance
nearzerovar <- nearZeroVar(modelDf, saveMetrics=TRUE)
sum(nearzerovar$zeroVar)
sum(nearzerovar$nzv)

#check for columns that may not add value to the model
names(modelDf[,c(1:7)])
```

The analysis shows that

* the variable 'classe' has 5 levels.  
* there are a large number of columns having all NA values and these need to be deleted because they may interfere with the model creation.
* there are no columns with 0 variance
* there are 60 columns with near zero variance.  These can also be deleted before creating the model.
* The first 7 coulumns are names and ids and can be removed.

# Data Cleanup

The data will be cleaned up to 

* delete columns which have a large percentage of NAs (over 60%)
* columns which have near zero variance and 
* the first 7 columns

```{r}
#remove columns with more than 60% NAs, 
#create logical vector for columns where the NA are less than 50%
naCount <- colSums(is.na(modelDf))/nrow(modelDf) < 0.4
#apply the vector on the dataframe to only select the columns where the % is less than 50%
modelDf <- modelDf[,naCount]
dim(modelDf)

#The first 7 columns may not add any value to the prediction
modelDf <- modelDf[,-c(1:7)]
dim(modelDf)

#remove columns whose variace is near zero
nearzerovar <- nearZeroVar(modelDf, saveMetrics=TRUE)
modelDf <- modelDf[,!nearzerovar$nzv]
dim(modelDf)

```

The number of variables has been reduced from 160 to 53.

## Splitting data for Cross validation

The dataset will be split into a training and testing data sets to do cross validation.

```{r}
#Split the training data into training and testing sets at ratio of 60:40 to do cross validation.
inTrain <- createDataPartition(modelDf$classe, p=0.6, list=FALSE)
training <- modelDf[inTrain,]
dim(training)
testing <- modelDf[-inTrain,]
dim(testing)

```

## Create a prediction model using the Classification Tree
```{r}
#Run the Classification Tree 
fitTree <- train(classe ~ ., data=training, method='rpart')
#Predict the classifications for testing dataset
predictTree <- predict(fitTree, testing)
#Create confusion matrix to see the accuracy
confusionMatrix(predictTree, testing$classe)

```


## Create a prediction model using Random Forest
```{r }
#Do the same for random forest
fitRf <- train(classe ~ ., data=training, method='rf')
predictRf <- predict(fitRf, testing)
confusionMatrix(predictRf, testing$classe)

```

The above shows that there is an accuracy for around 50% for Classification Tree and around 99% for Random Forest.  This shows that Random Forest prediction is better by far and can be  used to predict the 20 test cases.

#Final Predictions

The final prediction is performed by applying the above created Random Forest model to the 'pml-testing.csv' which contains the 20 test cases.

```{r}
# Choose the best model out of the list and predict using the model and the testing file.
finalPredictions <- predict(fitRf, predictDf)
finalPredictions

#Add the predictions to the testing data set and display it
predictDf <- cbind(predictDf, classe.predicted =finalPredictions)
predictDf[,c("user_name", "problem_id", "classe.predicted")]

```

The final predictions are displayed above.




